{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification of five different kinds of flowers by their images.\n",
    "\n",
    "The data set contains partitions of images of different flowers. These are {daisy, dadelion, rose, sunflower, tulip}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "import os, shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from PIL import Image # image module is a submodule of PIL that contains the classes and functions for working with individual images\n",
    "\n",
    "from keras_preprocessing.image import img_to_array\n",
    "from keras.layers import Dropout, Flatten,Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the classes/categories as per dataset\n",
    "categories=['daisy', 'rose', 'tulip', 'dandelion', 'sunflower']\n",
    "directory = 'Flowers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images\n",
    "images_count = []\n",
    "for category in os.listdir(directory):\n",
    "    images_count.append(len(os.listdir(r\"\" + directory + \"/\" + category)))\n",
    "images_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for category in os.listdir(directory):\n",
    "    for images in os.listdir(os.path.join(directory, category))[:733]:\n",
    "        image = Image.open(os.path.join(directory, category, images))\n",
    "        image = image.resize((150, 150))\n",
    "        image_array = img_to_array(image) / 255\n",
    "        data.append(image_array)\n",
    "        labels.append(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of labels\n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "labels = keras.utils.to_categorical(labels, len(np.unique(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation/test\n",
    "X_train, X_tval, y_train, y_tval = train_test_split(data, labels, stratify = labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the validation/test into validation and test data (50% of 30% = val and test each are 15% of the total dataset)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tval, y_tval, stratify = y_tval, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation on train data\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "X_train_augmented, y_train_augmented = datagen.flow(X_train, y_train, shuffle = True, batch_size=len(X_train)).next()\n",
    "\n",
    "X_train = np.concatenate((X_train, X_train_augmented))\n",
    "y_train = np.concatenate((y_train, y_train_augmented))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After adding the augmented images to the train dataset -> shuffle again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(X_train.shape[0])\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    model.add(keras.layers.Dense(512, activation='relu', \n",
    "    kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "batch_sizes = [10, 16, 32, 64, 128]\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "callbacks = [early_stop]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    model = get_model()\n",
    "    history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=100,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size = batch_size,\n",
    "            callbacks = callbacks)\n",
    "    val_acc = history.history[\"val_accuracy\"][-1]\n",
    "    df = df.append({'batch size' : batch_size,\n",
    "                    'validation accuracy': val_acc}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "testplan = df.append({'batch size' : batch_size,\n",
    "                'validation accuracy': val_acc,\n",
    "                'test accuracy': test_acc,\n",
    "                'test loss': test_loss }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('path/to/saved/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_images, test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
